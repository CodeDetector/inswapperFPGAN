{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-05-22 11:24:03.414212539 [E:onnxruntime:Default, provider_bridge_ort.cc:1532 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1209 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_max_tuning_duration_ms': '0', 'use_ep_level_unified_stream': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_external_alloc': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'has_user_compute_stream': '0', 'gpu_mem_limit': '18446744073709551615', 'device_id': '0'}}\n",
      "find model: ./checkpoints/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-05-22 11:24:04.256694004 [E:onnxruntime:Default, provider_bridge_ort.cc:1532 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1209 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[1;31m2024-05-22 11:24:04.293880706 [E:onnxruntime:Default, provider_bridge_ort.cc:1532 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1209 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n",
      "\u001b[1;31m2024-05-22 11:24:04.414929195 [E:onnxruntime:Default, provider_bridge_ort.cc:1532 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1209 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_max_tuning_duration_ms': '0', 'use_ep_level_unified_stream': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_external_alloc': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'has_user_compute_stream': '0', 'gpu_mem_limit': '18446744073709551615', 'device_id': '0'}}\n",
      "find model: ./checkpoints/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_max_tuning_duration_ms': '0', 'use_ep_level_unified_stream': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_external_alloc': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'has_user_compute_stream': '0', 'gpu_mem_limit': '18446744073709551615', 'device_id': '0'}}\n",
      "find model: ./checkpoints/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_max_tuning_duration_ms': '0', 'use_ep_level_unified_stream': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_external_alloc': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'has_user_compute_stream': '0', 'gpu_mem_limit': '18446744073709551615', 'device_id': '0'}}\n",
      "find model: ./checkpoints/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-05-22 11:24:04.449947446 [E:onnxruntime:Default, provider_bridge_ort.cc:1532 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1209 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.8: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_max_tuning_duration_ms': '0', 'use_ep_level_unified_stream': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_external_alloc': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'has_user_compute_stream': '0', 'gpu_mem_limit': '18446744073709551615', 'device_id': '0'}}\n",
      "find model: ./checkpoints/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (320, 320)\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'prefer_nhwc': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_max_tuning_duration_ms': '0', 'use_ep_level_unified_stream': '0', 'tunable_op_enable': '0', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'tunable_op_tuning_enable': '0', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_external_alloc': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'has_user_compute_stream': '0', 'gpu_mem_limit': '18446744073709551615', 'device_id': '0'}}\n",
      "inswapper-shape: [1, 3, 128, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/ubuntu/anaconda3/envs/env/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 0\n",
      "Restoring image 0\n",
      "Saving image 0\n",
      "Processing image 1\n",
      "Restoring image 1\n",
      "Saving image 1\n",
      "Processing image 2\n",
      "Restoring image 2\n",
      "Saving image 2\n",
      "Processing image 3\n",
      "Restoring image 3\n",
      "Saving image 3\n",
      "Processing image 4\n",
      "Restoring image 4\n",
      "Saving image 4\n",
      "Processing image 5\n",
      "Restoring image 5\n",
      "Saving image 5\n",
      "Processing image 6\n",
      "Restoring image 6\n",
      "Saving image 6\n",
      "Processing image 7\n",
      "Restoring image 7\n",
      "Saving image 7\n",
      "Processing image 8\n",
      "Restoring image 8\n",
      "Saving image 8\n",
      "Processing image 9\n",
      "Restoring image 9\n",
      "Saving image 9\n",
      "Processing image 10\n",
      "Restoring image 10\n",
      "Saving image 10\n",
      "Processing image 11\n",
      "Restoring image 11\n",
      "Saving image 11\n",
      "Processing image 12\n",
      "Restoring image 12\n",
      "Saving image 12\n",
      "Processing image 13\n",
      "Restoring image 13\n",
      "Saving image 13\n",
      "Processing image 14\n",
      "Restoring image 14\n",
      "Saving image 14\n",
      "Processing image 15\n",
      "Restoring image 15\n",
      "Saving image 15\n",
      "Processing image 16\n",
      "Restoring image 16\n",
      "Saving image 16\n",
      "Processing image 17\n",
      "Restoring image 17\n",
      "Saving image 17\n",
      "Processing image 18\n",
      "Restoring image 18\n",
      "Saving image 18\n",
      "Processing image 19\n",
      "Restoring image 19\n",
      "Saving image 19\n",
      "Processing image 20\n",
      "Restoring image 20\n",
      "Saving image 20\n",
      "Processing image 21\n",
      "Restoring image 21\n",
      "Saving image 21\n",
      "Processing image 22\n",
      "Restoring image 22\n",
      "Saving image 22\n",
      "Processing image 23\n",
      "Restoring image 23\n",
      "Saving image 23\n",
      "Processing image 24\n",
      "Restoring image 24\n",
      "Saving image 24\n",
      "Processing image 25\n",
      "Restoring image 25\n",
      "Saving image 25\n",
      "Processing image 26\n",
      "Restoring image 26\n",
      "Saving image 26\n",
      "Processing image 27\n",
      "Restoring image 27\n",
      "Saving image 27\n",
      "Processing image 28\n",
      "Restoring image 28\n",
      "Saving image 28\n",
      "Processing image 29\n",
      "Restoring image 29\n",
      "Saving image 29\n",
      "Processing image 30\n",
      "Restoring image 30\n",
      "Saving image 30\n",
      "Processing image 31\n",
      "Restoring image 31\n",
      "Saving image 31\n",
      "Processing image 32\n",
      "Restoring image 32\n",
      "Saving image 32\n",
      "Processing image 33\n",
      "Restoring image 33\n",
      "Saving image 33\n",
      "Processing image 34\n",
      "Restoring image 34\n",
      "Saving image 34\n",
      "Processing image 35\n",
      "Restoring image 35\n",
      "Saving image 35\n",
      "Processing image 36\n",
      "Restoring image 36\n",
      "Saving image 36\n",
      "Processing image 37\n",
      "Restoring image 37\n",
      "Saving image 37\n",
      "Processing image 38\n",
      "Restoring image 38\n",
      "Saving image 38\n",
      "Processing image 39\n",
      "Restoring image 39\n",
      "Saving image 39\n",
      "Processing image 40\n",
      "Restoring image 40\n",
      "Saving image 40\n",
      "Processing image 41\n",
      "Restoring image 41\n",
      "Saving image 41\n",
      "Processing image 42\n",
      "Restoring image 42\n",
      "Saving image 42\n",
      "Processing image 43\n",
      "Restoring image 43\n",
      "Saving image 43\n",
      "Processing image 44\n",
      "Restoring image 44\n",
      "Saving image 44\n",
      "Processing image 45\n",
      "Restoring image 45\n",
      "Saving image 45\n",
      "Processing image 46\n",
      "Restoring image 46\n",
      "Saving image 46\n",
      "Processing image 47\n",
      "Restoring image 47\n",
      "Saving image 47\n",
      "Processing image 48\n",
      "Restoring image 48\n",
      "Saving image 48\n",
      "Processing image 49\n",
      "Restoring image 49\n",
      "Saving image 49\n",
      "Processing image 50\n",
      "Restoring image 50\n",
      "Saving image 50\n",
      "Processing image 51\n",
      "Restoring image 51\n",
      "Saving image 51\n",
      "Processing image 52\n",
      "Restoring image 52\n",
      "Saving image 52\n",
      "Processing image 53\n",
      "Restoring image 53\n",
      "Saving image 53\n",
      "Processing image 54\n",
      "Restoring image 54\n",
      "Saving image 54\n",
      "Processing image 55\n",
      "Restoring image 55\n",
      "Saving image 55\n",
      "Processing image 56\n",
      "Restoring image 56\n",
      "Saving image 56\n",
      "Processing image 57\n",
      "Restoring image 57\n",
      "Saving image 57\n",
      "Processing image 58\n",
      "Restoring image 58\n",
      "Saving image 58\n",
      "Processing image 59\n",
      "Restoring image 59\n",
      "Saving image 59\n",
      "Processing image 60\n",
      "Restoring image 60\n",
      "Saving image 60\n",
      "Processing image 61\n",
      "Restoring image 61\n",
      "Saving image 61\n",
      "Processing image 62\n",
      "Restoring image 62\n",
      "Saving image 62\n",
      "Processing image 63\n",
      "Restoring image 63\n",
      "Saving image 63\n",
      "Processing image 64\n",
      "Restoring image 64\n",
      "Saving image 64\n",
      "Processing image 65\n",
      "Restoring image 65\n",
      "Saving image 65\n",
      "Processing image 66\n",
      "Restoring image 66\n",
      "Saving image 66\n",
      "Processing image 67\n",
      "Restoring image 67\n",
      "Saving image 67\n",
      "Processing image 68\n",
      "Restoring image 68\n",
      "Saving image 68\n",
      "Processing image 69\n",
      "Restoring image 69\n",
      "Saving image 69\n",
      "Processing image 70\n",
      "Restoring image 70\n",
      "Saving image 70\n",
      "Processing image 71\n",
      "Restoring image 71\n",
      "Saving image 71\n",
      "Processing image 72\n",
      "Restoring image 72\n",
      "Saving image 72\n",
      "Processing image 73\n",
      "Restoring image 73\n",
      "Saving image 73\n",
      "Processing image 74\n",
      "Restoring image 74\n",
      "Saving image 74\n",
      "Processing image 75\n",
      "Restoring image 75\n",
      "Saving image 75\n",
      "Processing image 76\n",
      "Restoring image 76\n",
      "Saving image 76\n",
      "Processing image 77\n",
      "Restoring image 77\n",
      "Saving image 77\n",
      "Processing image 78\n",
      "Restoring image 78\n",
      "Saving image 78\n",
      "Processing image 79\n",
      "Restoring image 79\n",
      "Saving image 79\n",
      "Processing image 80\n",
      "Restoring image 80\n",
      "Saving image 80\n",
      "Processing image 81\n",
      "Restoring image 81\n",
      "Saving image 81\n",
      "Processing image 82\n",
      "Restoring image 82\n",
      "Saving image 82\n",
      "Processing image 83\n",
      "Restoring image 83\n",
      "Saving image 83\n",
      "Processing image 84\n",
      "Restoring image 84\n",
      "Saving image 84\n",
      "Processing image 85\n",
      "Restoring image 85\n",
      "Saving image 85\n",
      "Processing image 86\n",
      "Restoring image 86\n",
      "Saving image 86\n",
      "Processing image 87\n",
      "Restoring image 87\n",
      "Saving image 87\n",
      "Processing image 88\n",
      "Restoring image 88\n",
      "Saving image 88\n",
      "Processing image 89\n",
      "Restoring image 89\n",
      "Saving image 89\n",
      "Processing image 90\n",
      "Restoring image 90\n",
      "Saving image 90\n",
      "Processing image 91\n",
      "Restoring image 91\n",
      "Saving image 91\n",
      "Processing image 92\n",
      "Restoring image 92\n",
      "Saving image 92\n",
      "Processing image 93\n",
      "Restoring image 93\n",
      "Saving image 93\n",
      "Processing image 94\n",
      "Restoring image 94\n",
      "Saving image 94\n",
      "Processing image 95\n",
      "Restoring image 95\n",
      "Saving image 95\n",
      "Processing image 96\n",
      "Restoring image 96\n",
      "Saving image 96\n",
      "Processing image 97\n",
      "Restoring image 97\n",
      "Saving image 97\n",
      "Processing image 98\n",
      "Restoring image 98\n",
      "Saving image 98\n",
      "Processing image 99\n",
      "Restoring image 99\n",
      "Saving image 99\n",
      "Processing image 100\n",
      "Restoring image 100\n",
      "Saving image 100\n",
      "Processing image 101\n",
      "Restoring image 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image 101\n",
      "Processing image 102\n",
      "Restoring image 102\n",
      "Saving image 102\n",
      "Processing image 103\n",
      "Restoring image 103\n",
      "Saving image 103\n",
      "Processing image 104\n",
      "Restoring image 104\n",
      "Saving image 104\n",
      "Processing image 105\n",
      "Restoring image 105\n",
      "Saving image 105\n",
      "Processing image 106\n",
      "Restoring image 106\n",
      "Saving image 106\n",
      "Processing image 107\n",
      "Restoring image 107\n",
      "Saving image 107\n",
      "Processing image 108\n",
      "Restoring image 108\n",
      "Saving image 108\n",
      "Processing image 109\n",
      "Restoring image 109\n",
      "Saving image 109\n",
      "Processing image 110\n",
      "Restoring image 110\n",
      "Saving image 110\n",
      "Processing image 111\n",
      "Restoring image 111\n",
      "Saving image 111\n",
      "Processing image 112\n",
      "Restoring image 112\n",
      "Saving image 112\n",
      "Processing image 113\n",
      "Restoring image 113\n",
      "Saving image 113\n",
      "Processing image 114\n",
      "Restoring image 114\n",
      "Saving image 114\n",
      "Processing image 115\n",
      "Restoring image 115\n",
      "Saving image 115\n",
      "Processing image 116\n",
      "Restoring image 116\n",
      "Saving image 116\n",
      "Processing image 117\n",
      "Restoring image 117\n",
      "Saving image 117\n",
      "Processing image 118\n",
      "Restoring image 118\n",
      "Saving image 118\n",
      "Processing image 119\n",
      "Restoring image 119\n",
      "Saving image 119\n",
      "Processing image 120\n",
      "Restoring image 120\n",
      "Saving image 120\n",
      "Processing image 121\n",
      "Restoring image 121\n",
      "Saving image 121\n",
      "Processing image 122\n",
      "Restoring image 122\n",
      "Saving image 122\n",
      "Processing image 123\n",
      "Restoring image 123\n",
      "Saving image 123\n",
      "Processing image 124\n",
      "Restoring image 124\n",
      "Saving image 124\n",
      "Processing image 125\n",
      "Restoring image 125\n",
      "Saving image 125\n",
      "Processing image 126\n",
      "Restoring image 126\n",
      "Saving image 126\n",
      "Processing image 127\n",
      "Restoring image 127\n",
      "Saving image 127\n",
      "Processing image 128\n",
      "Restoring image 128\n",
      "Saving image 128\n",
      "Processing image 129\n",
      "Restoring image 129\n",
      "Saving image 129\n",
      "Processing image 130\n",
      "Restoring image 130\n",
      "Saving image 130\n",
      "Processing image 131\n",
      "Restoring image 131\n",
      "Saving image 131\n",
      "Processing image 132\n",
      "Restoring image 132\n",
      "Saving image 132\n",
      "Processing image 133\n",
      "Restoring image 133\n",
      "Saving image 133\n",
      "Processing image 134\n",
      "Restoring image 134\n",
      "Saving image 134\n",
      "Processing image 135\n",
      "Restoring image 135\n",
      "Saving image 135\n",
      "Processing image 136\n",
      "Restoring image 136\n",
      "Saving image 136\n",
      "Processing image 137\n",
      "Restoring image 137\n",
      "Saving image 137\n",
      "Processing image 138\n",
      "Restoring image 138\n",
      "Saving image 138\n",
      "Processing image 139\n",
      "Restoring image 139\n",
      "Saving image 139\n",
      "Processing image 140\n",
      "Restoring image 140\n",
      "Saving image 140\n",
      "Processing image 141\n",
      "Restoring image 141\n",
      "Saving image 141\n",
      "Processing image 142\n",
      "Restoring image 142\n",
      "Saving image 142\n",
      "Processing image 143\n",
      "Restoring image 143\n",
      "Saving image 143\n",
      "Processing image 144\n",
      "Restoring image 144\n",
      "Saving image 144\n",
      "Processing image 145\n",
      "Restoring image 145\n",
      "Saving image 145\n",
      "Processing image 146\n",
      "Restoring image 146\n",
      "Saving image 146\n",
      "Processing image 147\n",
      "Restoring image 147\n",
      "Saving image 147\n",
      "Processing image 148\n",
      "Restoring image 148\n",
      "Saving image 148\n",
      "Processing image 149\n",
      "Restoring image 149\n",
      "Saving image 149\n",
      "Processing image 150\n",
      "Restoring image 150\n",
      "Saving image 150\n",
      "Processing image 151\n",
      "Restoring image 151\n",
      "Saving image 151\n",
      "Processing image 152\n",
      "Restoring image 152\n",
      "Saving image 152\n",
      "Processing image 153\n",
      "Restoring image 153\n",
      "Saving image 153\n",
      "Processing image 154\n",
      "Restoring image 154\n",
      "Saving image 154\n",
      "Processing image 155\n",
      "Restoring image 155\n",
      "Saving image 155\n",
      "Processing image 156\n",
      "Restoring image 156\n",
      "Saving image 156\n",
      "Processing image 157\n",
      "Restoring image 157\n",
      "Saving image 157\n",
      "Processing image 158\n",
      "Restoring image 158\n",
      "Saving image 158\n",
      "Processing image 159\n",
      "Restoring image 159\n",
      "Saving image 159\n",
      "Processing image 160\n",
      "Restoring image 160\n",
      "Saving image 160\n",
      "Processing image 161\n",
      "Restoring image 161\n",
      "Saving image 161\n",
      "Processing image 162\n",
      "Restoring image 162\n",
      "Saving image 162\n",
      "Processing image 163\n",
      "Restoring image 163\n",
      "Saving image 163\n",
      "Processing image 164\n",
      "Restoring image 164\n",
      "Saving image 164\n",
      "Processing image 165\n",
      "Restoring image 165\n",
      "Saving image 165\n",
      "Processing image 166\n",
      "Restoring image 166\n",
      "Saving image 166\n",
      "Processing image 167\n",
      "Restoring image 167\n",
      "Saving image 167\n",
      "Processing image 168\n",
      "Restoring image 168\n",
      "Saving image 168\n",
      "Processing image 169\n",
      "Restoring image 169\n",
      "Saving image 169\n",
      "Processing image 170\n",
      "Restoring image 170\n",
      "Saving image 170\n",
      "Processing image 171\n",
      "Restoring image 171\n",
      "Saving image 171\n",
      "Processing image 172\n",
      "Restoring image 172\n",
      "Saving image 172\n",
      "Processing image 173\n",
      "Restoring image 173\n",
      "Saving image 173\n",
      "Processing image 174\n",
      "Restoring image 174\n",
      "Saving image 174\n",
      "Processing image 175\n",
      "Restoring image 175\n",
      "Saving image 175\n",
      "Processing image 176\n",
      "Restoring image 176\n",
      "Saving image 176\n",
      "Processing image 177\n",
      "Restoring image 177\n",
      "Saving image 177\n",
      "Processing image 178\n",
      "Restoring image 178\n",
      "Saving image 178\n",
      "Processing image 179\n",
      "Restoring image 179\n",
      "Saving image 179\n",
      "Processing image 180\n",
      "Restoring image 180\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m predict \u001b[38;5;241m=\u001b[39m Predictor() \n\u001b[1;32m     12\u001b[0m predict\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mpredict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_imgs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefer_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreference_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inswapperFPGAN/predict.py:76\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, target_image, refer_image, video)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     target_image: Path \u001b[38;5;241m=\u001b[39m Input(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget input image\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     73\u001b[0m     refer_image : Path \u001b[38;5;241m=\u001b[39m Input(description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference input image\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     74\u001b[0m     video : Path \u001b[38;5;241m=\u001b[39m Input(description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo input \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[0;32m---> 76\u001b[0m     result_image \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_image\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefer_image\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_upsampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_analyser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyser\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_swapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswapper\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhancer\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/*.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(files, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, x)[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/inswapperFPGAN/swapper.py:171\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path, target_img_path, reference_img_path, model, restore, bg_upsampler, face_analyser, face_swapper, restorer)\u001b[0m\n\u001b[1;32m    169\u001b[0m img \u001b[38;5;241m=\u001b[39m process_image(frame , target_faces , reference_faces\u001b[38;5;241m=\u001b[39mreference_faces , face_analyser\u001b[38;5;241m=\u001b[39m face_analyser , face_swapper\u001b[38;5;241m=\u001b[39m face_swapper , restore \u001b[38;5;241m=\u001b[39m restore )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestoring image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mbg_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrestorer\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m    173\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,img)\n",
      "File \u001b[0;32m~/inswapperFPGAN/gfp/main.py:12\u001b[0m, in \u001b[0;36mbg_sampler\u001b[0;34m(input_img, restorer, bg_upsampler)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbg_sampler\u001b[39m(input_img  ,restorer ,  bg_upsampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  ):         \n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# restore faces and background if necessary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 12\u001b[0m         _, _, restored_img \u001b[38;5;241m=\u001b[39m \u001b[43mrestorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhas_aligned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43monly_center_face\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpaste_back\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m restored_img\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inswapperFPGAN/gfp/utils.py:120\u001b[0m, in \u001b[0;36mGFPGANer.enhance\u001b[0;34m(self, img, has_aligned, only_center_face, paste_back, weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_helper\u001b[38;5;241m.\u001b[39mget_inverse_affine(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# paste each restored face to the input image\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     restored_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaste_faces_to_input_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsample_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_helper\u001b[38;5;241m.\u001b[39mcropped_faces, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_helper\u001b[38;5;241m.\u001b[39mrestored_faces, restored_img\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/env/lib/python3.10/site-packages/facexlib/utils/face_restoration_helper.py:355\u001b[0m, in \u001b[0;36mFaceRestoreHelper.paste_faces_to_input_image\u001b[0;34m(self, save_path, upsample_img)\u001b[0m\n\u001b[1;32m    353\u001b[0m         upsample_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((upsample_img, alpha), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         upsample_img \u001b[38;5;241m=\u001b[39m inv_soft_mask \u001b[38;5;241m*\u001b[39m pasted_face \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m inv_soft_mask) \u001b[38;5;241m*\u001b[39m upsample_img\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(upsample_img) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m256\u001b[39m:  \u001b[38;5;66;03m# 16-bit image\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     upsample_img \u001b[38;5;241m=\u001b[39m upsample_img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint16)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from swapper import process_video  , create_video \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from predict import Predictor \n",
    "\n",
    "model = \"./checkpoints/inswapper_128_fp16.onnx\"\n",
    "video_path = \"./Daawat World Biryani  Offline 60 Sec 19.05.24.mp4\"\n",
    "reference_images = [\"./WhatsApp Image 2024-05-20 at 19.17.24_2baa6ee8.jpg\"] \n",
    "target_imgs= [\"./Screenshot 2024-05-20 101516.png\"]\n",
    "\n",
    "predict = Predictor() \n",
    "predict.setup()\n",
    "predict.predict(target_image= target_imgs[0] , refer_image= reference_images[0] , video= video_path ) \n",
    "# result_image = process_video(video_path, target_imgs , reference_images , model  , restore = True , bg_upsampler = False  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restored = [] \n",
    "# for frame in result_image : \n",
    "# #    print(f\"Restoring : {idx} image\" )\n",
    "#     restored.append(restore(frame) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import glob \n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "files = glob.glob('images/*.png')\n",
    "files = sorted(files, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "files = [os.path.basename(p) for p in files]\n",
    "frames = [] \n",
    "\n",
    "h ,w , _ = cv2.imread(\"/home/ubuntu/inswapperFPGAN/images/images0.png\").shape \n",
    "    \n",
    "def create_video(h , w ) : \n",
    " \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(\"output1.mp4\", fourcc,25, (w, h))\n",
    "    # print(\"Wahts jmwifndfvnio\")\n",
    "    # Write each image to the video\n",
    "    for image in files:\n",
    "            image = cv2.imread(\"images/\"+image)\n",
    "            out.write(image)\n",
    "    out.release()\n",
    "create_video(h, w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
